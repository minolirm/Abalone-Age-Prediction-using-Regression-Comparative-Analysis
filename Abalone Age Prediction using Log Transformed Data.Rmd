---
title: Regression Modelling to Predict the Age of the Abaolne based on its Physical
  Characteristics - Log Transformed Response
author: "Minoli Munasinghe"
date: "2023-04-10"
output:
  pdf_document: default
  html_document: default
---

Read the dataset from UCI machine learning repository

```{r}
# Read the dataset into a data frame
abalone_df <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header=FALSE)
# Print first five rows in the data set
head(abalone_df)
```
Add column names to the data set

```{r}
# add column names
names(abalone_df) <- c("sex", "length", "diameter", "height", "weight.whole",
                    "weight.shucked", "weight.viscera", "weight.shell", "rings")
```

Get the number of observations in the data

```{r}
# numbers of rows in the data set
nrow(abalone_df)
```

Get the number of columns in the data
```{r}
ncol(abalone_df)
```
The data set has 4177 observations with 9 variables

Get the number of missing values

```{r}
# Check for the missing values
sum(is.na(abalone_df))
```
The data set does not have any missing values

Get the structure of data

```{r}
str(abalone_df)
```
According to the data, there is one categorical variable (sex) and all the oher variables are continuous. The response variable of the model is rings.

The distribution of response variable

```{r}
library(ggplot2)
ggplot(aes(x = rings),data = abalone_df)+
geom_histogram(binwidth = 0.5,fill="navy blue") +
scale_x_continuous(breaks = abalone_df$rings)+
ylab("Abalone Frequency ")+
xlab("Number of Rings")+
ggtitle("The Frequency Distribution of Abalone Rings")+
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
```
```{r}
library(moments)
skewness(abalone_df$rings)
```

The distribution of observations by number of rings is observed using the above histogram and according to its skewness value of 1.1, we can consider that it is moderately positively skewed and the distribution is also not symmetrically distributed around the mean. 


```{r}
library(ggplot2)
ggplot(aes(x = sex),data = abalone_df)+
  geom_bar(stat="count",width = 0.3,fill = "dark orange")+
  theme(legend.position = "None")+
  ggtitle("Sex Distribution of abalone")
```
Observe the distribution of other variables

```{r}
boxplot(abalone_df[,c(2,3,4,5,6,8)],col = "dark orange") # boxplot to check possible outliers
```

According to the boxplots drawn for all the other variables, we can see that all variables have outlier values in the data.

Correlation plot to observe the relationship between variables

```{r}

library(GGally)
ggcorr(abalone_df[,c(2,3,4,5,6,8,9)], label = T, label_size = 2.9, hjust = 1, layout.exp = 3) +
  scale_fill_gradient(low = "dark orange", high = "navy blue")
 

```

According to the correlation plot, we can see that the predictor variables are highly correlated and therefore multicollinearity exists.

Pairwise Scatterplot

```{r}
# Plot the scatter matrix
pairs(abalone_df[,2:9], main = "Scatterplot Matrix of Abalone Data",col = "dark orange",cex=0.1,cex.main = 0.9)
```
```{r}
abalone_df$trn_rings = log(abalone_df$rings+1.495)
skewness(abalone_df$trn_rings)
```

The distribution of Log transformation of the rings

```{r}
library(ggplot2)
ggplot(aes(x = trn_rings),data = abalone_df)+
geom_histogram(fill="Navy blue")+
xlab("Number of log(rings)")
ggtitle("The Frequency Distribution of Log of Abalone Rings")+
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
```

Handling the outlier data

```{r}

find_outliers = function(x){
  q1 = quantile(x,probs = 0.25)
  q3 = quantile(x,probs = 0.75)
  
  IQR = q3 - q1
  
  status = x < q1 - (IQR*1.5) | x > q3 + (IQR*1.5)

}

remove_outliers=function(processed_data,cols = names(processed_data)) {

    for (col in cols) {
        processed_data = processed_data[!find_outliers(processed_data[[col]]), ]
    } 
  return(processed_data)
}

abalone_df = remove_outliers(abalone_df,cols = 
                                   c("length","diameter","height","weight.whole",
                                    "weight.shucked","weight.viscera","weight.shell"))
dim(abalone_df)
```
According to the results, current dataset without outliers has 4013 records, thus only 164 records have been removed due to outlier detection.

Convert the categorical variable  *sex* into numeric variable

```{r}
# Convert sex to factor
sex_factor <- as.factor(abalone_df$sex)

# Convert factor levels to numeric values
abalone_df["sex"] = as.numeric(sex_factor)

```

Data Normalization

```{r}
# Standard Scaling the data
abalone_df = scale(abalone_df,scale = TRUE,center = TRUE)

```

Split the data set in to training and testing set

```{r}

set.seed(2023)

# Remove the 9th column (rings) from the dataset
abalone_df <- subset(abalone_df, select = -rings)

# Split data into training & testing set
sample <- sample(c(TRUE, FALSE), nrow(abalone_df), replace=TRUE, prob=c(0.80,0.20))
train_set <- abalone_df[sample, ]
test_set <- abalone_df[!sample, ]

# Separate predictor and response data from training set
x_train <- train_set[,-9]
y_train <- train_set[,9]

# Separate predictor and response data from testing set
x_test <- test_set[,-9]
y_test <- test_set[,9]

```


Fit the multiple linear regression model with training data including all predictor variables
(Transformed response variable - log(rings))

```{r}

# -------------------------------Multiple Linear Regression --------------------------------------------

# The start time to train the model
start = Sys.time()
# Train Multiple Linear regression model
ml_model <- lm(trn_rings~., data = as.data.frame(train_set))
# End time to finish the training
end = Sys.time()
# Time taken to train the model
mlr.time = end-start

summary(ml_model)
```
The adjusted R squared value of training set is 56.2% with a RMSE of 66.7%. According to the summary results, length and diameter are not significant variables of the model.

Making predictions from the trained multiple linear regression model on the testing set

```{r}
library(ModelMetrics)

# R squared on train data
mlr.tr.Rsquare = summary(ml_model)$adj.r.squared
# RMSE on train data
mlr.tr.RMSE = 0.6678

# Predict on test data
y_pred <- predict(ml_model, newdata=as.data.frame(x_test))

# RMSE and R squared for test set
data.frame(
mlr.te.RMSE = caret::RMSE(pred, y_test),
mlr.te.Rsquare = caret::R2(pred,y_test)
)
```


```{r}
plot(ml_model, col = "navy blue",cex = 0.8,pch=20)
```

We can see that the assumptions of the linear regression is violated and therefore, we first need to remove multicollinearity among the predictor variables, so that the correlation among the predictors will be reduced to a considerable amount to get a better fitted model

To remove the multicolinearity, I would choose principal component regressor, ridge regualarization and lasso regualrization

PCR - Reducing the dimensions would transform the correlated predictor variables into uncorrelated principal components which would reduce the multicolinearity. 

The optimum number of components in the PCR is obtained from hyper parameter tuning


```{r}
# Principal Component Regression with hyper parameter tuning using 10-fold cross validation

library(caret)

# Start time to train the PCR
start = Sys.time()

# define hyperparameters for the number of components 
hyperparams <- expand.grid(ncomp = c(1, 2, 3, 4,5,6,7,8)) 

# Train PCR model using cross validation
pcr.model <- train(trn_rings~., data = as.data.frame(train_set), method = "pcr",
                  scale = TRUE,
                  trControl = trainControl("cv", number = 10),
                  tuneGrid=hyperparams
)
# End time to finish the training
end = Sys.time()
# Time taken to train the model
pcr.time_taken = end-start

```


```{r}
# Print the best tuning parameter (number of components) that minimize the cross-validation error, RMSE
pcr.model$bestTune
```

```{r}
# Summarize the final PCR model
summary(pcr.model$finalModel)

```
According to the results, optimum number of components selected is 8, and 56% of the variability in the log rings can be explained by the predictor variables.

Making predictions on the training and testing data

```{r}
library(dplyr)

# Get the R squared and RMSE on training set
train_pred <- pcr.model %>% predict(x_train)
data.frame(
pcr.tr.RMSE = caret::RMSE(train_pred, y_train),
pcr.tr.Rsquare = caret::R2(train_pred,y_train)
)

# Make predictions on test data
test_pred <- pcr.model %>% predict(x_test)
data.frame(
pcr.te.RMSE = caret::RMSE(test_pred, y_test),
pcr.te.Rsquare = caret::R2(test_pred,y_test)
)

```

Ridge Regression model

A ridge regression model is fitted with hyperparameter tuning of lambda using 10-fold cross validation

```{r}

library(glmnet)

# Start time
start  = Sys.time()

# define grid of hyper parameters
grid <- expand.grid(alpha = 0, lambda = seq(0, 1, by = 0.01))

# implement 10-fold cross validation
ctrl <- trainControl(method = "cv", number = 10)

# train the ridge regression model using 10-fold cross validation
ridge.model <- train(trn_rings ~ ., data = train_set, method = "glmnet", trControl = ctrl, tuneGrid = grid)

# Optimized alpha
ridge.model$bestTune$alpha

# Optimized lambda
ridge.model$bestTune$lambda

# Fit the final model on the training data using optimal lambda
ridge.model <- glmnet(x = as.matrix(x_train), y = as.matrix(y_train), alpha = 0, lambda = ridge.model$bestTune$lambda)

# End time
end = Sys.time()
# Time taken to train the ridge model
ridge.time_taken = end - start

```

Making predictions on the training data and testing data using ridge regression model

```{r}
library(dplyr)

# Make predictions on train_set
train_pred <- ridge.model %>% predict(as.matrix(x_train))
data.frame(
ridge.tr.RMSE = caret::RMSE(train_pred, y_train),
ridge.tr.Rsquared = caret::R2(train_pred,y_train)
)


# Make predictions on the test data
y_test = as.matrix(y_test)
test_pred <- ridge.model %>% predict(as.matrix(x_test))
data.frame(
ridge.te.RMSE = caret::RMSE(test_pred, y_test),
ridge.te.Rsquared = caret::R2(test_pred,y_test)
)

```

Lasso Regression

Lasso Regressor is fitted with hyper parameter tuning of lambda using 10-fold cross validation

```{r}

# Start time
start = Sys.time()

# Train the lasso model 
cv.lasso <- cv.glmnet(x = x_train, y = as.matrix(y_train), alpha =1, lambda = NULL)
plot(cv.lasso)

# Optimal lambda
cv.lasso$lambda.min

# Fit the final model on the training data using optimal lambda
lasso.model <- glmnet(x = x_train, y = as.matrix(y_train), alpha = 1, lambda = cv.lasso$lambda.min)

# End time
end = Sys.time()
# Time taken to train the model
lasso.time_taken = end - start
```

The optimal lambda obtained from hyper parameter tuning for lasso regression model is 0.00029

The trained model on optimum lambda is used to make the predictions on training and testing data

```{r}
train_pred <- lasso.model %>% predict(x_train)
data.frame(
lasso.tr.RMSE = caret::RMSE(train_pred, y_train),
lasso.tr.Rsquared = caret::R2(train_pred,y_train)
)


# Make predictions on the test data

predictions <- lasso.model %>% predict(x_test)
data.frame(
lasso.te.RMSE = caret::RMSE(predictions, y_test),
lasso.te.Rsquared = caret::R2(predictions,y_test)
)
```
Support Vector Regressor

A support vector regressor is fitted with tuning the hyperparameters (C,Sigma) using 10-fold cross validation
```{r}

#Load Library
library(e1071)
library(caret)

# Start time
start = Sys.time()
# Define grid for hyper parameter tuning
grid <- expand.grid(sigma = c(0.01, 0.1, 1),
                    C = c(0.1, 1, 10))

# train svm model with cross validation & defined grid of hyper parameters
ctrl <- trainControl(method = "cv", number = 10)
svm.model <- train(trn_rings ~ ., data = train_set, method = "svmRadial", trControl = ctrl, tuneGrid = grid)


# Optimum parameters
best_c <- svm.model$bestTune$C
best_sigma <- svm.model$bestTune$sigma

# train svm model with optimum parameters
svm.model <- train(trn_rings ~ ., data = train_set, kernel = "radial", cost = best_c, epsilon = best_sigma)

# end time
end = Sys.time()
# Time taken to train
svm.time_taken = end - start

# Prediction on training data
train_pred = predict(svm.model, x_train)
data.frame(
  svm.tr.RMSE = caret::RMSE(train_pred, y_train),
  svm.tr.Rsquare = caret::R2(train_pred, y_train)
)
#Prediction on testing data
test_pred = predict(svm.model, x_test)
data.frame(
  svm.te.RMSE = caret::RMSE(test_pred, y_test),
  svm.te.Rsquare = caret::R2(test_pred, y_test)
)

# Optimum parameters
best_c
best_sigma
```

This is computationally expensive method.

Random Forest Regressor


```{r}
# ---------------------------------------Random Forest Regressor ---------------------------
library(caret)


# Start time
start = Sys.time()

train.control <- trainControl(method = "cv", number = 10)
# Train the model
rf.model <- train(trn_rings ~., data = train_set, method = "rf",
                      trControl = train.control,
                      tuneLength=20)
print(rf.model) # Summarize the results
plot(rf.model, col = "dark orange")

# end time
end = Sys.time()
# Execution time for RF
rf.time = end-start

# Make predictions on train data
train_pred <- rf.model %>% predict(x_train)

data.frame(
  RMSE = caret::RMSE(train_pred, y_train),
  Rsquare = caret::R2(train_pred, y_train)
)

# Make predictions on test data
test_pred <- rf.model %>% predict(x_test)

data.frame(
  RMSE = caret::RMSE(test_pred, y_test),
  Rsquare = caret::R2(test_pred, y_test)
)
varImp(rf.model)
varImp(rf.model,conditional = TRUE)
```

Decision Tree Regressor

```{r}
# Load libraries
library(rpart)
library(rpart.plot)
library(caret)

# Set up cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Define hyperparameter grid
grid <- expand.grid(cp = seq(0, 0.1, by = 0.01))


# Start time
start = Sys.time()

# Tune hyperparameters using cross-validation
dt_model <- train(trn_rings ~ ., 
                  data = train_set, 
                  method = "rpart",
                  trControl = ctrl,
                  tuneGrid = grid)

# Print the best parameters
print(dt_model$bestTune)

# Train the final model using the best parameters
final_dt_model <- rpart(trn_rings ~ ., 
                        data = as.data.frame(train_set),
                        cp = dt_model$bestTune$cp)

# end time
end = Sys.time()
# Execution time for RF
dt.time = end-start

# Plot the tree
rpart.plot(final_dt_model)

# Make predictions on train data
train_pred <- final_dt_model %>% predict(as.data.frame(x_train))

data.frame(
  RMSE = caret::RMSE(train_pred, y_train),
  Rsquare = caret::R2(train_pred, y_train)
)

# Make predictions on test data
test_pred <- final_dt_model %>% predict(as.data.frame(x_test))

data.frame(
  RMSE = caret::RMSE(test_pred, y_test),
  Rsquare = caret::R2(test_pred, y_test)
)
```

Neural Network

```{r}

library(nnet)

# Start time
start = Sys.time()

ctrl <- trainControl(method = "cv", number = 10,
                     search = "grid", summaryFunction = defaultSummary)

grid <- expand.grid(size = c(5,6,7,8), decay = seq(0.1, 0.2, 0.3))

model <- train(trn_rings ~ ., data = train_set, method = "nnet", tuneGrid = grid, 
               trControl = ctrl)

best_size = model$bestTune$size
best_decay = model$bestTune$decay

nn.model <- nnet(trn_rings ~ ., data = train_set, size = best_size, decay = best_decay, maxit = 1000)

# end time
end = Sys.time()
# Execution time for RF
nn.time = end-start

# Make predictions on train data
train_pred <- nn.model %>% predict(as.data.frame(x_train))

data.frame(
  RMSE = caret::RMSE(train_pred, y_train),
  Rsquare = caret::R2(train_pred, y_train)
)

# Make predictions on test data
test_pred <- nn.model %>% predict(as.data.frame(x_test))

data.frame(
  RMSE = caret::RMSE(test_pred, y_test),
  Rsquare = caret::R2(test_pred, y_test)
)
```


Bayesian Regression


```{r}
suppressPackageStartupMessages(library(rstanarm))

# Start time
start = Sys.time()

model_bayes<- stan_glm(trn_rings~., data=as.data.frame(train_set), family = gaussian(),
prior=normal(0, 5),seed=111)

end = Sys.time()
bayes.time = end-start
```

```{r}
summary(model_bayes)
```

posterior_predictive checks


```{r}
y_pred = predict(model_bayes,newdata = as.data.frame(x_test))
data.frame(
  RMSE = caret::RMSE(y_pred, y_test),
  Rsquare = caret::R2(y_pred, y_test)
)

y_pred_train = predict(model_bayes,newdata = as.data.frame(x_train))
data.frame(
  RMSE = caret::RMSE(y_pred_train, y_train),
  Rsquare = caret::R2(y_pred_train, y_train)
)
```
The execution time of each model
```{r}
print("Multiple Linear Regression Model")
mlr.time
print("Principal Component Regressor")
pcr.time_taken
print("Ridge Regressor")
ridge.time_taken
print("Lasso Regressor")
lasso.time_taken
print("Support Vector Regressor")
svm.time_taken
print("Random Forest Regressor")
rf.time
print("Decision Tree Regressor")
dt.time
print("Neural Network")
nn.time
print("Bayesian Regression")
bayes.time

```

Boxplot representation of RMSE values of each model

```{r}
RMSE_data <- data.frame(Model = c("MLR", "PCR", "Ridge","Lasso","SVM","DT","RF","NN","Bayes"),
                      RMSE = c(0.6818485, 0.6760124	, 0.6887168,0.6758897,0.6301614	,0.6295017,0.7247068,0.8469765,0.6669065))

ggplot(RMSE_data, aes(x = Model, y = RMSE)) + geom_boxplot(col = "navy blue") + ylab("RMSE - 10-fold CV")
```
The R squared values of all the models are obtained in a boxplot to obtain a clear understanding about the performance of the model.

```{r}
R2_data <- data.frame(Model = c("MLR", "PCR", "Ridge","Lasso","SVM","DT","RF","NN","Bayes"),
                      Rsquare = c(0.4968994, 0.5064648	, 0.4870175,0.5065215,0.5715283	,	0.4373023,0.572148,0.3973273,0.5629431))

ggplot(R2_data, aes(x = Model, y = Rsquare)) + geom_boxplot(col = "navy blue") + ylab("R-square - 10-fold CV")
```

