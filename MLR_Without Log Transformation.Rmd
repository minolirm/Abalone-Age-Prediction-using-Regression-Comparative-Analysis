---
title: Regression Modelling to Predict the Age of the Abaolne based on its Physical
  Characteristics
author: "Minoli Munasinghe"
date: "2023-04-10"
output:
  pdf_document: default
  html_document: default
---

Read the dataset from UCI machine learning repository

```{r}
# Read the dataset into a data frame
abalone_df <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", header=FALSE)
# Print first five rows in the data set
head(abalone_df)
```
Add column names to the data set

```{r}
# add column names
names(abalone_df) <- c("sex", "length", "diameter", "height", "weight.whole",
                    "weight.shucked", "weight.viscera", "weight.shell", "rings")
```

Get the number of observations in the data

```{r}
# numbers of rows in the data set
nrow(abalone_df)
```

Get the number of columns in the data
```{r}
ncol(abalone_df)
```
The data set has 4177 observations with 9 variables

Get the number of missing values

```{r}
# Check for the missing values
sum(is.na(abalone_df))
```
The data set does not have any missing values

Get the structure of data

```{r}
str(abalone_df)
```

According to the data, there is one categorical variable (sex) and all the oher variables are continuous. The response variable of the model is rings.

The distribution of response variable

```{r}
library(ggplot2)
ggplot(aes(x = rings),data = abalone_df)+
geom_histogram(binwidth = 0.5,fill="navy blue") +
scale_x_continuous(breaks = abalone_df$rings)+
ylab("Abalone Frequency")+
xlab("Number of Rings")+
ggtitle("The Frequency Distribution of Abalone Rings")+
theme_classic() +
theme(plot.title = element_text(hjust = 0.5))
```


```{r}
find_outliers = function(x){
  q1 = quantile(x,probs = 0.25)
  q3 = quantile(x,probs = 0.75)
  
  IQR = q3 - q1
  
  status = x < q1 - (IQR*1.5) | x > q3 + (IQR*1.5)

}

remove_outliers=function(processed_data,cols = names(processed_data)) {

    for (col in cols) {
        processed_data = processed_data[!find_outliers(processed_data[[col]]), ]
    } 
  return(processed_data)
}

abalone_df = remove_outliers(abalone_df,cols = 
                                   c("length","diameter","height","weight.whole",
                                    "weight.shucked","weight.viscera","weight.shell"))
dim(abalone_df)
```

```{r}
# Convert sex to factor
sex_factor <- as.factor(abalone_df$sex)

# Convert factor levels to numeric values
abalone_df["sex"] = as.numeric(sex_factor)

```



Data Normalization

```{r}
abalone_df = scale(abalone_df,scale = TRUE,center = TRUE)

```

Split the dataset in to training and testing set

```{r}

set.seed(2023)

# Split data into training & testing
sample <- sample(c(TRUE, FALSE), nrow(abalone_df), replace=TRUE, prob=c(0.80,0.20))
train_set <- abalone_df[sample, ]
test_set <- abalone_df[!sample, ]

# Separate predictor and response data from training set
x_train <- train_set[,-9]
y_train <- train_set[,9]

# Separate predictor and response data from testing set
x_test <- test_set[,-9]
y_test <- test_set[,9]

```


```{r}
# ------Multiple Linear Regression model--------------------------------------------
library(car)

ml_model <- lm(rings~., data = as.data.frame(train_set))
summary(ml_model)

vif(ml_model)
```
```{r}
plot(ml_model)
```
According to the plots, the multiple linear regression model assumptios are not satisfied.

Since the weight.whole variable have high  VIF value, I will remove that from the predictor variables and fit the model again

```{r}
ml_model_2 <- lm(rings~.-weight.whole, data = as.data.frame(train_set))
summary(ml_model_2)
```

However, removing the highly correlated predictors increase the RMSE and decrease the R squared values. Therefore, log transformation of the rings variable is used to predict the models

